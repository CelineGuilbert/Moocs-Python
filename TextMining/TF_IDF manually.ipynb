{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La formule TF-IDF permet de déterminer dans quelles proportions certains mots d’un document texte, d'un corps de document ou d’un site web peuvent être évalués par rapport au reste du texte.\n",
    "\n",
    "https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "if you like yuna and tomate sauce- try combining the two.\n",
    "It's really not as bad as it sounds.\n",
    "If the Easter Bunny and the Tooth Fairy had babies would they take \n",
    "your teeth and leave chocolate for you?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function remove_string_special_characters removes special characters from within a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s):\n",
    "    #replace special character with ' '\n",
    "    stripped = re.sub([^\\w\\s]','',s)\n",
    "    stripped = re.sub('_','',stripped)\n",
    "    #changed any whitespace to one space                \n",
    "    stripped = re.sub('\\s+',' ', stripped)\n",
    "    #remove start and end whitespace\n",
    "    stripped = stripped.strip()\n",
    "                      \n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function get_doc splits the text into sentences and considering each sentence as a DOCUMENT, calculates the total word count of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(sent):\n",
    "    \n",
    "    doc_info = []\n",
    "    i=0\n",
    "    for sent in text_sents_clean:\n",
    "            i+=1\n",
    "            count=count_words(sent)\n",
    "            temp={'doc_id':i, 'doc_length':count}\n",
    "            doc_info.append(temp)\n",
    "    return doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function count_words returns the total number of words in the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sent):\n",
    "    count=0\n",
    "    words=word_tokenize(sent)\n",
    "    for word in words:\n",
    "        count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function create_freq_dict creates a frequency dictionary for each word in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_freq_dict(sents):\n",
    "    i=0\n",
    "    freqDict_list=[]\n",
    "    for sent in sents:\n",
    "        i+=1\n",
    "        freq_dict={}\n",
    "        words=word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word=word.lower()\n",
    "            if word in freq_dict:\n",
    "                freq_dict[word]+=1\n",
    "            else:\n",
    "                freq_dict[word] =1\n",
    "            temp={'doc_id' :i, 'freq_dict': freq_dict}\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF détermine la fréquence relative d’un mot ou d’une combinaison de mots dans un documen\n",
    "\n",
    "\n",
    "TF = (Frequency of the term in the doc / total number of terms in the doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(doc_info, freqDist_list):\n",
    "    TF_score=[]\n",
    "    for tempDict in freqDict_list:\n",
    "        id=tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp={'doc_id':id,\n",
    "                   'TF_score: tempDict['freq_dict'][k]/doc_info[id-1]['doc_length'],\n",
    "                  'key' : k'}\n",
    "            TF_scores.append(temp)\n",
    "    return TF_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L’IDF calcule le Inverse Document Frequency (la fréquence inverse du document) et complète l’analyse de l’évaluation du mot. Il agit en tant que correctif du TF. \n",
    "#### L’IDF inclut dans le calcul la fréquence des documents pour un mot précis, autrement dit l’IDF compare le chiffre correspondant à tous les documents connus avec le nombre de textes contenant le mot en question\n",
    "\n",
    "\n",
    "IDF = ln(total number of docs/number of docs with terms in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(doc_info, freqDict_list):\n",
    "    IDF_scores=[]\n",
    "    counter=0\n",
    "    for dict in freqDict_list:\n",
    "        counter+=1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count=sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp={'doc_id': counter, 'IDF_score': math.log(len(doc_info)/count),'key' : k}\n",
    "            IDF_scores.append(temp)\n",
    "    return IDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    TFIDF_scores=[]\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key']==i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp={'doc_id':j['doc_id'],\n",
    "                     'TFIDF_score':j['IDF_score']*i['TF_score'],\n",
    "                     'key': i['key']}\n",
    "        TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\n000187816/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\n000187816\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\n000187816\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\n000187816\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\n000187816\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-abaa202393ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\n000187816/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\n000187816\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\n000187816\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\n000187816\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\n000187816\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text_sents = sent_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
